{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorise the title/content into known set of topic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the parent directory to the module search path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the data\n",
    "mini_data_file_path = '../data/mini_data.csv'\n",
    "data = pd.read_csv(mini_data_file_path)\n",
    "\n",
    "# Combine 'title' and 'content' columns into a single text column\n",
    "data['text'] = data['title'] + ' ' + data['content']\n",
    "\n",
    "# Preprocess the text data using NLTK\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "\n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'text' column\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Get the preprocessed text data\n",
    "preprocessed_text_data = data['preprocessed_text']\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "X = vectorizer.fit_transform(preprocessed_text_data)\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Apply LDA model to the vectorized data\n",
    "n_topics = 5  # Number of topics/categories\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the most important words for each topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
    "    top_words_per_topic.append(top_words)\n",
    "\n",
    "# Assign categories to the topics\n",
    "categories = ['Sports', 'Politics', 'Technology', 'Entertainment', 'Health']\n",
    "\n",
    "# Print the topics and their associated categories\n",
    "for topic_idx, top_words in enumerate(top_words_per_topic):\n",
    "    category = categories[topic_idx]\n",
    "    print(f'Topic {topic_idx + 1}: {\", \".join(top_words)} (Category: {category})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics and Trends Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the parent directory to the module search path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the data\n",
    "mini_data_file_path = '../data/mini_data.csv'\n",
    "data = pd.read_csv(mini_data_file_path)\n",
    "\n",
    "# Combine 'title' and 'content' columns into a single text column\n",
    "data['text'] = data['title'] + ' ' + data['content']\n",
    "\n",
    "# Preprocess the text data using NLTK\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords]\n",
    "\n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocess_text function to the 'text' column\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Get the preprocessed text data\n",
    "preprocessed_text_data = data['preprocessed_text']\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "X = vectorizer.fit_transform(preprocessed_text_data)\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Apply LDA model to the vectorized data\n",
    "n_topics = 5  # Number of topics/categories\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "topic_weights = lda.fit_transform(X)\n",
    "\n",
    "# Calculate the diversity of topics covered by different websites\n",
    "data['topic_diversity'] = topic_weights.std(axis=1)\n",
    "\n",
    "# Get the unique websites\n",
    "websites = data['website'].unique()\n",
    "\n",
    "# Sort the websites based on topic diversity\n",
    "sorted_websites = sorted(websites, key=lambda w: data[data['website'] == w]['topic_diversity'].mean(), reverse=True)\n",
    "\n",
    "# Print the websites with the most diverse topics\n",
    "print(\"Websites with the most diverse topics:\")\n",
    "for website in sorted_websites[:5]:\n",
    "    print(website)\n",
    "\n",
    "# Plot the 2D scatter plot of topic trends over time\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Initialize the color map\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for topic_idx in range(n_topics):\n",
    "    topic = f\"Topic {topic_idx + 1}\"\n",
    "    topic_counts = data.groupby(['date', 'topic_diversity'])[topic].sum()\n",
    "    color = [cmap(i) for i in topic_counts]\n",
    "    plt.scatter(topic_counts.index.get_level_values('date'), [topic_idx] * len(topic_counts), c=color, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Topics')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Topic Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
